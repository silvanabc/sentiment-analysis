{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Feature Extraction\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_PATH = 'extractTest/text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a sample of the text file\n",
    "\n",
    "# textual = \\\n",
    "# [\"Let's compare the camera capabilities of the iphone eleven pro and \", \n",
    "# \"and the google pixel\",\n",
    "# \"pixel four\",\n",
    "# \"how is everyone doing today? It's Andrew here from Apple Insider and we have our google pixel four\",\n",
    "# \"and iphone eleven pro and iphone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the word2vec from Google model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model available in https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the utterances into embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TXT_PATH, 'r') as file:\n",
    "    txt = file.read()\n",
    "\n",
    "utterances = txt.split('\\n')\n",
    "\n",
    "# utterances = textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_vectors = []\n",
    "for u in utterances:\n",
    "    words = u.strip().split(\" \")\n",
    "    word_vectors = []\n",
    "    errors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            v = model[word]\n",
    "            word_vectors.append(v)\n",
    "        except:\n",
    "            errors.append(word)\n",
    "    utterance_vectors.append(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding word count for each utterance: [9, 3, 2, 16, 4]\n"
     ]
    }
   ],
   "source": [
    "len_utterances = []\n",
    "for u in utterance_vectors:\n",
    "    len_utterances.append(len(u))\n",
    "print(\"Embedding word count for each utterance:\", len_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utterances in a window of 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_words_vector(words_vector, size=SIZE):\n",
    "\n",
    "    before_len = int((size - len(words_vector))/2)\n",
    "    after_len = size - len(words_vector) - before_len\n",
    "    \n",
    "    words_vector = np.array(words_vector)\n",
    "\n",
    "    return np.pad(words_vector, ((before_len, after_len),(0,0)), 'wrap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_vectors = []\n",
    "for u in utterances:\n",
    "    words = u.strip().split(\" \")\n",
    "    word_vectors = []\n",
    "    errors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            v = model[word]\n",
    "            word_vectors.append(v)\n",
    "        except:\n",
    "            errors.append(word)\n",
    "    utterance_vectors.append(wrap_words_vector(word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(utterance_vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = max(map(len, utterance_vectors)) #length as the max of the array\n",
    "length = 50\n",
    "fill_value = None\n",
    "\n",
    "wrapped_array = np.array([xi+[fill_value]*(length-len(xi)) for xi in utterance_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
