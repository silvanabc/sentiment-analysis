{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "## Kinects-i3D ##\n",
    "import sys\n",
    "sys.path.insert(1, '../kinetics-i3d/') #insert the kinects-i3d project to the path\n",
    "import i3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_CLASSES = 400\n",
    "_IMAGE_SIZE = [224,224]\n",
    "_FRAMES = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_array(video_path):\n",
    "    clip = VideoFileClip(video_path, target_resolution=(_IMAGE_SIZE[0],_IMAGE_SIZE[1]))\n",
    "    frames = np.array([x for x in clip.iter_frames()])\n",
    "    return pad_frames(frames)\n",
    "    \n",
    "def pad_frames(frames):\n",
    "    frames_qtt = frames.shape[0]\n",
    "    if(frames_qtt < _FRAMES): #padding the frame\n",
    "\n",
    "        pad_left_count = int((_FRAMES - frames_qtt) / 2)\n",
    "        pad_right_count = _FRAMES - frames_qtt - pad_left_count\n",
    "\n",
    "        pad_left = np.zeros((pad_left_count, frames.shape[1],  frames.shape[2],  frames.shape[3]))\n",
    "        pad_right = np.zeros((pad_right_count, frames.shape[1],  frames.shape[2],  frames.shape[3]))\n",
    "\n",
    "        rgb_array = np.concatenate((pad_left, frames, pad_right))\n",
    "\n",
    "#         print('Array padded')\n",
    "\n",
    "    else: \n",
    "        ##TODO: reduce the array -- CHECK IT!\n",
    "        rgb_array = np.resize(frames.mean(axis=0).astype(int),\n",
    "                              (_FRAMES, frames.shape[1],  frames.shape[2],  frames.shape[3]))\n",
    "#         print('Array resized')\n",
    "\n",
    "    return rgb_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an array of utterances frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utterances_array(utterances_path, start=1):\n",
    "    result_array = np.empty((0, _FRAMES, _IMAGE_SIZE[0],_IMAGE_SIZE[1], 3))\n",
    "    count = start \n",
    "    while(True):\n",
    "        try:\n",
    "            f = get_frames_array(utterances_path + str(count) + \".mp4\")\n",
    "            result_array = np.append(result_array, [f], axis=0)\n",
    "            count +=1\n",
    "    #         if(count == 10):\n",
    "    #             break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"{0} utterances processed\".format(count-start))\n",
    "    #         print(e)\n",
    "            break\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 utterances processed\n"
     ]
    }
   ],
   "source": [
    "path = \"../MOSI_Dataset/Segmented/_dI--eQ6qVU_\"\n",
    "\n",
    "rgb_array = get_utterances_array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 64, 224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 224, 224, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expand the array in one more dimension\n",
    "# rgb_array = np.expand_dims(rgb_array, axis=0)\n",
    "# rgb_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_visual_features(rgb_array):    \n",
    "    i3d_model = i3d.InceptionI3d(num_classes=_NUM_CLASSES, final_endpoint='Predictions')\n",
    "\n",
    "    inp = tf.placeholder(tf.float32, [None, _FRAMES, _IMAGE_SIZE[0], _IMAGE_SIZE[1], 3])\n",
    "\n",
    "    predictions, end_points = i3d_model(inp, is_training=True, dropout_keep_prob=0.5)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # sample_input = np.zeros((5, 64, _IMAGE_SIZE[0], _IMAGE_SIZE[1], 3))\n",
    "    sample_input = rgb_array\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        out_predictions, out_logits = sess.run([predictions, end_points['Logits']], {inp: sample_input})\n",
    "    \n",
    "    return out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_logits = model_visual_features(rgb_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save video features in a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../MOSI_Dataset/ProcessedData/\"\n",
    "filename = '_dI--eQ6qVU'\n",
    "np.save(path + filename, out_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0776252 ,  0.52176446, -0.57059944, ...,  0.57569677,\n",
       "        -0.08314126,  0.2847538 ],\n",
       "       [-0.20371394,  0.45799956, -0.433181  , ...,  0.5871268 ,\n",
       "         0.1312544 ,  0.4346002 ],\n",
       "       [-0.18181585,  0.435361  , -0.48127335, ...,  0.5568002 ,\n",
       "        -0.13335773,  0.32280993],\n",
       "       ...,\n",
       "       [-0.20457196,  0.5942516 , -0.5102663 , ...,  0.6374935 ,\n",
       "        -0.03212039,  0.12488917],\n",
       "       [-0.10982757,  0.61451787, -0.58562803, ...,  0.6420568 ,\n",
       "        -0.40021592,  0.46122852],\n",
       "       [-0.14870346,  0.662376  , -0.16642144, ...,  0.5224863 ,\n",
       "        -0.14700334,  0.2605955 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(path + filename + '.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
